/* 
 * SHA-1 hash in x86 assembly
 * 
 * Copyright (c) 2014 Project Nayuki
 * http://www.nayuki.io/page/fast-sha1-hash-implementation-in-x86-assembly
 * 
 * (MIT License)
 * Permission is hereby granted, free of charge, to any person obtaining a copy of
 * this software and associated documentation files (the "Software"), to deal in
 * the Software without restriction, including without limitation the rights to
 * use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
 * the Software, and to permit persons to whom the Software is furnished to do so,
 * subject to the following conditions:
 * - The above copyright notice and this permission notice shall be included in
 *   all copies or substantial portions of the Software.
 * - The Software is provided "as is", without warranty of any kind, express or
 *   implied, including but not limited to the warranties of merchantability,
 *   fitness for a particular purpose and noninfringement. In no event shall the
 *   authors or copyright holders be liable for any claim, damages or other
 *   liability, whether in an action of contract, tort or otherwise, arising from,
 *   out of or in connection with the Software or the use or other dealings in the
 *   Software.
 */

# Modified by kerukuro for use in cppcrypto.

# 1 "sha1-fast.S"
# 1 "<built-in>"
# 1 "<command-line>"
# 1 "sha1-fast.S"
# 27 "sha1-fast.S"
.globl sha1_compress
.globl _sha1_compress
sha1_compress:
_sha1_compress:
# 107 "sha1-fast.S"
 subl $80, %esp
 movl %ebx, 64(%esp)
 movl %esi, 68(%esp)
 movl %edi, 72(%esp)
 movl %ebp, 76(%esp)


 movl 84(%esp), %esi
 movl 88(%esp), %edi
 movl 0(%esi), %eax
 movl 4(%esi), %ebx
 movl 8(%esi), %ecx
 movl 12(%esi), %edx
 movl 16(%esi), %ebp


 movl (0*4)(%edi), %esi; bswapl %esi; movl %esi, (0*4)(%esp); addl %esi, %ebp; movl %ecx, %esi; xorl %edx, %esi; andl %ebx, %esi; xorl %edx, %esi; roll $30, %ebx; leal 0x5A827999(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (1*4)(%edi), %esi; bswapl %esi; movl %esi, (1*4)(%esp); addl %esi, %edx; movl %ebx, %esi; xorl %ecx, %esi; andl %eax, %esi; xorl %ecx, %esi; roll $30, %eax; leal 0x5A827999(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (2*4)(%edi), %esi; bswapl %esi; movl %esi, (2*4)(%esp); addl %esi, %ecx; movl %eax, %esi; xorl %ebx, %esi; andl %ebp, %esi; xorl %ebx, %esi; roll $30, %ebp; leal 0x5A827999(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (3*4)(%edi), %esi; bswapl %esi; movl %esi, (3*4)(%esp); addl %esi, %ebx; movl %ebp, %esi; xorl %eax, %esi; andl %edx, %esi; xorl %eax, %esi; roll $30, %edx; leal 0x5A827999(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (4*4)(%edi), %esi; bswapl %esi; movl %esi, (4*4)(%esp); addl %esi, %eax; movl %edx, %esi; xorl %ebp, %esi; andl %ecx, %esi; xorl %ebp, %esi; roll $30, %ecx; leal 0x5A827999(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (5*4)(%edi), %esi; bswapl %esi; movl %esi, (5*4)(%esp); addl %esi, %ebp; movl %ecx, %esi; xorl %edx, %esi; andl %ebx, %esi; xorl %edx, %esi; roll $30, %ebx; leal 0x5A827999(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (6*4)(%edi), %esi; bswapl %esi; movl %esi, (6*4)(%esp); addl %esi, %edx; movl %ebx, %esi; xorl %ecx, %esi; andl %eax, %esi; xorl %ecx, %esi; roll $30, %eax; leal 0x5A827999(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (7*4)(%edi), %esi; bswapl %esi; movl %esi, (7*4)(%esp); addl %esi, %ecx; movl %eax, %esi; xorl %ebx, %esi; andl %ebp, %esi; xorl %ebx, %esi; roll $30, %ebp; leal 0x5A827999(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (8*4)(%edi), %esi; bswapl %esi; movl %esi, (8*4)(%esp); addl %esi, %ebx; movl %ebp, %esi; xorl %eax, %esi; andl %edx, %esi; xorl %eax, %esi; roll $30, %edx; leal 0x5A827999(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (9*4)(%edi), %esi; bswapl %esi; movl %esi, (9*4)(%esp); addl %esi, %eax; movl %edx, %esi; xorl %ebp, %esi; andl %ecx, %esi; xorl %ebp, %esi; roll $30, %ecx; leal 0x5A827999(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (10*4)(%edi), %esi; bswapl %esi; movl %esi, (10*4)(%esp); addl %esi, %ebp; movl %ecx, %esi; xorl %edx, %esi; andl %ebx, %esi; xorl %edx, %esi; roll $30, %ebx; leal 0x5A827999(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (11*4)(%edi), %esi; bswapl %esi; movl %esi, (11*4)(%esp); addl %esi, %edx; movl %ebx, %esi; xorl %ecx, %esi; andl %eax, %esi; xorl %ecx, %esi; roll $30, %eax; leal 0x5A827999(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (12*4)(%edi), %esi; bswapl %esi; movl %esi, (12*4)(%esp); addl %esi, %ecx; movl %eax, %esi; xorl %ebx, %esi; andl %ebp, %esi; xorl %ebx, %esi; roll $30, %ebp; leal 0x5A827999(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (13*4)(%edi), %esi; bswapl %esi; movl %esi, (13*4)(%esp); addl %esi, %ebx; movl %ebp, %esi; xorl %eax, %esi; andl %edx, %esi; xorl %eax, %esi; roll $30, %edx; leal 0x5A827999(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (14*4)(%edi), %esi; bswapl %esi; movl %esi, (14*4)(%esp); addl %esi, %eax; movl %edx, %esi; xorl %ebp, %esi; andl %ecx, %esi; xorl %ebp, %esi; roll $30, %ecx; leal 0x5A827999(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (15*4)(%edi), %esi; bswapl %esi; movl %esi, (15*4)(%esp); addl %esi, %ebp; movl %ecx, %esi; xorl %edx, %esi; andl %ebx, %esi; xorl %edx, %esi; roll $30, %ebx; leal 0x5A827999(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((16 - 3)&0xF)*4)(%esp), %esi; xorl (((16 - 8)&0xF)*4)(%esp), %esi; xorl (((16 -14)&0xF)*4)(%esp), %esi; xorl (((16 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((16&0xF)*4)(%esp); movl %ebx, %esi; xorl %ecx, %esi; andl %eax, %esi; xorl %ecx, %esi; roll $30, %eax; leal 0x5A827999(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((17 - 3)&0xF)*4)(%esp), %esi; xorl (((17 - 8)&0xF)*4)(%esp), %esi; xorl (((17 -14)&0xF)*4)(%esp), %esi; xorl (((17 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((17&0xF)*4)(%esp); movl %eax, %esi; xorl %ebx, %esi; andl %ebp, %esi; xorl %ebx, %esi; roll $30, %ebp; leal 0x5A827999(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((18 - 3)&0xF)*4)(%esp), %esi; xorl (((18 - 8)&0xF)*4)(%esp), %esi; xorl (((18 -14)&0xF)*4)(%esp), %esi; xorl (((18 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((18&0xF)*4)(%esp); movl %ebp, %esi; xorl %eax, %esi; andl %edx, %esi; xorl %eax, %esi; roll $30, %edx; leal 0x5A827999(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((19 - 3)&0xF)*4)(%esp), %esi; xorl (((19 - 8)&0xF)*4)(%esp), %esi; xorl (((19 -14)&0xF)*4)(%esp), %esi; xorl (((19 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((19&0xF)*4)(%esp); movl %edx, %esi; xorl %ebp, %esi; andl %ecx, %esi; xorl %ebp, %esi; roll $30, %ecx; leal 0x5A827999(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (((20 - 3)&0xF)*4)(%esp), %esi; xorl (((20 - 8)&0xF)*4)(%esp), %esi; xorl (((20 -14)&0xF)*4)(%esp), %esi; xorl (((20 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebp; movl %esi, ((20&0xF)*4)(%esp); movl %ebx, %esi; xorl %ecx, %esi; xorl %edx, %esi; roll $30, %ebx; leal 0x6ED9EBA1(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((21 - 3)&0xF)*4)(%esp), %esi; xorl (((21 - 8)&0xF)*4)(%esp), %esi; xorl (((21 -14)&0xF)*4)(%esp), %esi; xorl (((21 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((21&0xF)*4)(%esp); movl %eax, %esi; xorl %ebx, %esi; xorl %ecx, %esi; roll $30, %eax; leal 0x6ED9EBA1(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((22 - 3)&0xF)*4)(%esp), %esi; xorl (((22 - 8)&0xF)*4)(%esp), %esi; xorl (((22 -14)&0xF)*4)(%esp), %esi; xorl (((22 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((22&0xF)*4)(%esp); movl %ebp, %esi; xorl %eax, %esi; xorl %ebx, %esi; roll $30, %ebp; leal 0x6ED9EBA1(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((23 - 3)&0xF)*4)(%esp), %esi; xorl (((23 - 8)&0xF)*4)(%esp), %esi; xorl (((23 -14)&0xF)*4)(%esp), %esi; xorl (((23 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((23&0xF)*4)(%esp); movl %edx, %esi; xorl %ebp, %esi; xorl %eax, %esi; roll $30, %edx; leal 0x6ED9EBA1(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((24 - 3)&0xF)*4)(%esp), %esi; xorl (((24 - 8)&0xF)*4)(%esp), %esi; xorl (((24 -14)&0xF)*4)(%esp), %esi; xorl (((24 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((24&0xF)*4)(%esp); movl %ecx, %esi; xorl %edx, %esi; xorl %ebp, %esi; roll $30, %ecx; leal 0x6ED9EBA1(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (((25 - 3)&0xF)*4)(%esp), %esi; xorl (((25 - 8)&0xF)*4)(%esp), %esi; xorl (((25 -14)&0xF)*4)(%esp), %esi; xorl (((25 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebp; movl %esi, ((25&0xF)*4)(%esp); movl %ebx, %esi; xorl %ecx, %esi; xorl %edx, %esi; roll $30, %ebx; leal 0x6ED9EBA1(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((26 - 3)&0xF)*4)(%esp), %esi; xorl (((26 - 8)&0xF)*4)(%esp), %esi; xorl (((26 -14)&0xF)*4)(%esp), %esi; xorl (((26 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((26&0xF)*4)(%esp); movl %eax, %esi; xorl %ebx, %esi; xorl %ecx, %esi; roll $30, %eax; leal 0x6ED9EBA1(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((27 - 3)&0xF)*4)(%esp), %esi; xorl (((27 - 8)&0xF)*4)(%esp), %esi; xorl (((27 -14)&0xF)*4)(%esp), %esi; xorl (((27 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((27&0xF)*4)(%esp); movl %ebp, %esi; xorl %eax, %esi; xorl %ebx, %esi; roll $30, %ebp; leal 0x6ED9EBA1(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((28 - 3)&0xF)*4)(%esp), %esi; xorl (((28 - 8)&0xF)*4)(%esp), %esi; xorl (((28 -14)&0xF)*4)(%esp), %esi; xorl (((28 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((28&0xF)*4)(%esp); movl %edx, %esi; xorl %ebp, %esi; xorl %eax, %esi; roll $30, %edx; leal 0x6ED9EBA1(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((29 - 3)&0xF)*4)(%esp), %esi; xorl (((29 - 8)&0xF)*4)(%esp), %esi; xorl (((29 -14)&0xF)*4)(%esp), %esi; xorl (((29 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((29&0xF)*4)(%esp); movl %ecx, %esi; xorl %edx, %esi; xorl %ebp, %esi; roll $30, %ecx; leal 0x6ED9EBA1(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (((30 - 3)&0xF)*4)(%esp), %esi; xorl (((30 - 8)&0xF)*4)(%esp), %esi; xorl (((30 -14)&0xF)*4)(%esp), %esi; xorl (((30 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebp; movl %esi, ((30&0xF)*4)(%esp); movl %ebx, %esi; xorl %ecx, %esi; xorl %edx, %esi; roll $30, %ebx; leal 0x6ED9EBA1(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((31 - 3)&0xF)*4)(%esp), %esi; xorl (((31 - 8)&0xF)*4)(%esp), %esi; xorl (((31 -14)&0xF)*4)(%esp), %esi; xorl (((31 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((31&0xF)*4)(%esp); movl %eax, %esi; xorl %ebx, %esi; xorl %ecx, %esi; roll $30, %eax; leal 0x6ED9EBA1(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((32 - 3)&0xF)*4)(%esp), %esi; xorl (((32 - 8)&0xF)*4)(%esp), %esi; xorl (((32 -14)&0xF)*4)(%esp), %esi; xorl (((32 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((32&0xF)*4)(%esp); movl %ebp, %esi; xorl %eax, %esi; xorl %ebx, %esi; roll $30, %ebp; leal 0x6ED9EBA1(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((33 - 3)&0xF)*4)(%esp), %esi; xorl (((33 - 8)&0xF)*4)(%esp), %esi; xorl (((33 -14)&0xF)*4)(%esp), %esi; xorl (((33 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((33&0xF)*4)(%esp); movl %edx, %esi; xorl %ebp, %esi; xorl %eax, %esi; roll $30, %edx; leal 0x6ED9EBA1(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((34 - 3)&0xF)*4)(%esp), %esi; xorl (((34 - 8)&0xF)*4)(%esp), %esi; xorl (((34 -14)&0xF)*4)(%esp), %esi; xorl (((34 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((34&0xF)*4)(%esp); movl %ecx, %esi; xorl %edx, %esi; xorl %ebp, %esi; roll $30, %ecx; leal 0x6ED9EBA1(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (((35 - 3)&0xF)*4)(%esp), %esi; xorl (((35 - 8)&0xF)*4)(%esp), %esi; xorl (((35 -14)&0xF)*4)(%esp), %esi; xorl (((35 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebp; movl %esi, ((35&0xF)*4)(%esp); movl %ebx, %esi; xorl %ecx, %esi; xorl %edx, %esi; roll $30, %ebx; leal 0x6ED9EBA1(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((36 - 3)&0xF)*4)(%esp), %esi; xorl (((36 - 8)&0xF)*4)(%esp), %esi; xorl (((36 -14)&0xF)*4)(%esp), %esi; xorl (((36 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((36&0xF)*4)(%esp); movl %eax, %esi; xorl %ebx, %esi; xorl %ecx, %esi; roll $30, %eax; leal 0x6ED9EBA1(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((37 - 3)&0xF)*4)(%esp), %esi; xorl (((37 - 8)&0xF)*4)(%esp), %esi; xorl (((37 -14)&0xF)*4)(%esp), %esi; xorl (((37 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((37&0xF)*4)(%esp); movl %ebp, %esi; xorl %eax, %esi; xorl %ebx, %esi; roll $30, %ebp; leal 0x6ED9EBA1(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((38 - 3)&0xF)*4)(%esp), %esi; xorl (((38 - 8)&0xF)*4)(%esp), %esi; xorl (((38 -14)&0xF)*4)(%esp), %esi; xorl (((38 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((38&0xF)*4)(%esp); movl %edx, %esi; xorl %ebp, %esi; xorl %eax, %esi; roll $30, %edx; leal 0x6ED9EBA1(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((39 - 3)&0xF)*4)(%esp), %esi; xorl (((39 - 8)&0xF)*4)(%esp), %esi; xorl (((39 -14)&0xF)*4)(%esp), %esi; xorl (((39 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((39&0xF)*4)(%esp); movl %ecx, %esi; xorl %edx, %esi; xorl %ebp, %esi; roll $30, %ecx; leal 0x6ED9EBA1(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (((40 - 3)&0xF)*4)(%esp), %esi; xorl (((40 - 8)&0xF)*4)(%esp), %esi; xorl (((40 -14)&0xF)*4)(%esp), %esi; xorl (((40 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebp; movl %esi, ((40&0xF)*4)(%esp); movl %ecx, %esi; movl %ecx, %edi; orl %edx, %esi; andl %ebx, %esi; andl %edx, %edi; orl %edi, %esi; roll $30, %ebx; leal 0x8F1BBCDC(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((41 - 3)&0xF)*4)(%esp), %esi; xorl (((41 - 8)&0xF)*4)(%esp), %esi; xorl (((41 -14)&0xF)*4)(%esp), %esi; xorl (((41 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((41&0xF)*4)(%esp); movl %ebx, %esi; movl %ebx, %edi; orl %ecx, %esi; andl %eax, %esi; andl %ecx, %edi; orl %edi, %esi; roll $30, %eax; leal 0x8F1BBCDC(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((42 - 3)&0xF)*4)(%esp), %esi; xorl (((42 - 8)&0xF)*4)(%esp), %esi; xorl (((42 -14)&0xF)*4)(%esp), %esi; xorl (((42 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((42&0xF)*4)(%esp); movl %eax, %esi; movl %eax, %edi; orl %ebx, %esi; andl %ebp, %esi; andl %ebx, %edi; orl %edi, %esi; roll $30, %ebp; leal 0x8F1BBCDC(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((43 - 3)&0xF)*4)(%esp), %esi; xorl (((43 - 8)&0xF)*4)(%esp), %esi; xorl (((43 -14)&0xF)*4)(%esp), %esi; xorl (((43 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((43&0xF)*4)(%esp); movl %ebp, %esi; movl %ebp, %edi; orl %eax, %esi; andl %edx, %esi; andl %eax, %edi; orl %edi, %esi; roll $30, %edx; leal 0x8F1BBCDC(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((44 - 3)&0xF)*4)(%esp), %esi; xorl (((44 - 8)&0xF)*4)(%esp), %esi; xorl (((44 -14)&0xF)*4)(%esp), %esi; xorl (((44 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((44&0xF)*4)(%esp); movl %edx, %esi; movl %edx, %edi; orl %ebp, %esi; andl %ecx, %esi; andl %ebp, %edi; orl %edi, %esi; roll $30, %ecx; leal 0x8F1BBCDC(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (((45 - 3)&0xF)*4)(%esp), %esi; xorl (((45 - 8)&0xF)*4)(%esp), %esi; xorl (((45 -14)&0xF)*4)(%esp), %esi; xorl (((45 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebp; movl %esi, ((45&0xF)*4)(%esp); movl %ecx, %esi; movl %ecx, %edi; orl %edx, %esi; andl %ebx, %esi; andl %edx, %edi; orl %edi, %esi; roll $30, %ebx; leal 0x8F1BBCDC(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((46 - 3)&0xF)*4)(%esp), %esi; xorl (((46 - 8)&0xF)*4)(%esp), %esi; xorl (((46 -14)&0xF)*4)(%esp), %esi; xorl (((46 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((46&0xF)*4)(%esp); movl %ebx, %esi; movl %ebx, %edi; orl %ecx, %esi; andl %eax, %esi; andl %ecx, %edi; orl %edi, %esi; roll $30, %eax; leal 0x8F1BBCDC(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((47 - 3)&0xF)*4)(%esp), %esi; xorl (((47 - 8)&0xF)*4)(%esp), %esi; xorl (((47 -14)&0xF)*4)(%esp), %esi; xorl (((47 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((47&0xF)*4)(%esp); movl %eax, %esi; movl %eax, %edi; orl %ebx, %esi; andl %ebp, %esi; andl %ebx, %edi; orl %edi, %esi; roll $30, %ebp; leal 0x8F1BBCDC(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((48 - 3)&0xF)*4)(%esp), %esi; xorl (((48 - 8)&0xF)*4)(%esp), %esi; xorl (((48 -14)&0xF)*4)(%esp), %esi; xorl (((48 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((48&0xF)*4)(%esp); movl %ebp, %esi; movl %ebp, %edi; orl %eax, %esi; andl %edx, %esi; andl %eax, %edi; orl %edi, %esi; roll $30, %edx; leal 0x8F1BBCDC(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((49 - 3)&0xF)*4)(%esp), %esi; xorl (((49 - 8)&0xF)*4)(%esp), %esi; xorl (((49 -14)&0xF)*4)(%esp), %esi; xorl (((49 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((49&0xF)*4)(%esp); movl %edx, %esi; movl %edx, %edi; orl %ebp, %esi; andl %ecx, %esi; andl %ebp, %edi; orl %edi, %esi; roll $30, %ecx; leal 0x8F1BBCDC(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (((50 - 3)&0xF)*4)(%esp), %esi; xorl (((50 - 8)&0xF)*4)(%esp), %esi; xorl (((50 -14)&0xF)*4)(%esp), %esi; xorl (((50 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebp; movl %esi, ((50&0xF)*4)(%esp); movl %ecx, %esi; movl %ecx, %edi; orl %edx, %esi; andl %ebx, %esi; andl %edx, %edi; orl %edi, %esi; roll $30, %ebx; leal 0x8F1BBCDC(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((51 - 3)&0xF)*4)(%esp), %esi; xorl (((51 - 8)&0xF)*4)(%esp), %esi; xorl (((51 -14)&0xF)*4)(%esp), %esi; xorl (((51 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((51&0xF)*4)(%esp); movl %ebx, %esi; movl %ebx, %edi; orl %ecx, %esi; andl %eax, %esi; andl %ecx, %edi; orl %edi, %esi; roll $30, %eax; leal 0x8F1BBCDC(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((52 - 3)&0xF)*4)(%esp), %esi; xorl (((52 - 8)&0xF)*4)(%esp), %esi; xorl (((52 -14)&0xF)*4)(%esp), %esi; xorl (((52 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((52&0xF)*4)(%esp); movl %eax, %esi; movl %eax, %edi; orl %ebx, %esi; andl %ebp, %esi; andl %ebx, %edi; orl %edi, %esi; roll $30, %ebp; leal 0x8F1BBCDC(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((53 - 3)&0xF)*4)(%esp), %esi; xorl (((53 - 8)&0xF)*4)(%esp), %esi; xorl (((53 -14)&0xF)*4)(%esp), %esi; xorl (((53 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((53&0xF)*4)(%esp); movl %ebp, %esi; movl %ebp, %edi; orl %eax, %esi; andl %edx, %esi; andl %eax, %edi; orl %edi, %esi; roll $30, %edx; leal 0x8F1BBCDC(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((54 - 3)&0xF)*4)(%esp), %esi; xorl (((54 - 8)&0xF)*4)(%esp), %esi; xorl (((54 -14)&0xF)*4)(%esp), %esi; xorl (((54 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((54&0xF)*4)(%esp); movl %edx, %esi; movl %edx, %edi; orl %ebp, %esi; andl %ecx, %esi; andl %ebp, %edi; orl %edi, %esi; roll $30, %ecx; leal 0x8F1BBCDC(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (((55 - 3)&0xF)*4)(%esp), %esi; xorl (((55 - 8)&0xF)*4)(%esp), %esi; xorl (((55 -14)&0xF)*4)(%esp), %esi; xorl (((55 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebp; movl %esi, ((55&0xF)*4)(%esp); movl %ecx, %esi; movl %ecx, %edi; orl %edx, %esi; andl %ebx, %esi; andl %edx, %edi; orl %edi, %esi; roll $30, %ebx; leal 0x8F1BBCDC(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((56 - 3)&0xF)*4)(%esp), %esi; xorl (((56 - 8)&0xF)*4)(%esp), %esi; xorl (((56 -14)&0xF)*4)(%esp), %esi; xorl (((56 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((56&0xF)*4)(%esp); movl %ebx, %esi; movl %ebx, %edi; orl %ecx, %esi; andl %eax, %esi; andl %ecx, %edi; orl %edi, %esi; roll $30, %eax; leal 0x8F1BBCDC(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((57 - 3)&0xF)*4)(%esp), %esi; xorl (((57 - 8)&0xF)*4)(%esp), %esi; xorl (((57 -14)&0xF)*4)(%esp), %esi; xorl (((57 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((57&0xF)*4)(%esp); movl %eax, %esi; movl %eax, %edi; orl %ebx, %esi; andl %ebp, %esi; andl %ebx, %edi; orl %edi, %esi; roll $30, %ebp; leal 0x8F1BBCDC(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((58 - 3)&0xF)*4)(%esp), %esi; xorl (((58 - 8)&0xF)*4)(%esp), %esi; xorl (((58 -14)&0xF)*4)(%esp), %esi; xorl (((58 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((58&0xF)*4)(%esp); movl %ebp, %esi; movl %ebp, %edi; orl %eax, %esi; andl %edx, %esi; andl %eax, %edi; orl %edi, %esi; roll $30, %edx; leal 0x8F1BBCDC(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((59 - 3)&0xF)*4)(%esp), %esi; xorl (((59 - 8)&0xF)*4)(%esp), %esi; xorl (((59 -14)&0xF)*4)(%esp), %esi; xorl (((59 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((59&0xF)*4)(%esp); movl %edx, %esi; movl %edx, %edi; orl %ebp, %esi; andl %ecx, %esi; andl %ebp, %edi; orl %edi, %esi; roll $30, %ecx; leal 0x8F1BBCDC(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (((60 - 3)&0xF)*4)(%esp), %esi; xorl (((60 - 8)&0xF)*4)(%esp), %esi; xorl (((60 -14)&0xF)*4)(%esp), %esi; xorl (((60 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebp; movl %esi, ((60&0xF)*4)(%esp); movl %ebx, %esi; xorl %ecx, %esi; xorl %edx, %esi; roll $30, %ebx; leal 0xCA62C1D6(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((61 - 3)&0xF)*4)(%esp), %esi; xorl (((61 - 8)&0xF)*4)(%esp), %esi; xorl (((61 -14)&0xF)*4)(%esp), %esi; xorl (((61 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((61&0xF)*4)(%esp); movl %eax, %esi; xorl %ebx, %esi; xorl %ecx, %esi; roll $30, %eax; leal 0xCA62C1D6(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((62 - 3)&0xF)*4)(%esp), %esi; xorl (((62 - 8)&0xF)*4)(%esp), %esi; xorl (((62 -14)&0xF)*4)(%esp), %esi; xorl (((62 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((62&0xF)*4)(%esp); movl %ebp, %esi; xorl %eax, %esi; xorl %ebx, %esi; roll $30, %ebp; leal 0xCA62C1D6(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((63 - 3)&0xF)*4)(%esp), %esi; xorl (((63 - 8)&0xF)*4)(%esp), %esi; xorl (((63 -14)&0xF)*4)(%esp), %esi; xorl (((63 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((63&0xF)*4)(%esp); movl %edx, %esi; xorl %ebp, %esi; xorl %eax, %esi; roll $30, %edx; leal 0xCA62C1D6(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((64 - 3)&0xF)*4)(%esp), %esi; xorl (((64 - 8)&0xF)*4)(%esp), %esi; xorl (((64 -14)&0xF)*4)(%esp), %esi; xorl (((64 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((64&0xF)*4)(%esp); movl %ecx, %esi; xorl %edx, %esi; xorl %ebp, %esi; roll $30, %ecx; leal 0xCA62C1D6(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (((65 - 3)&0xF)*4)(%esp), %esi; xorl (((65 - 8)&0xF)*4)(%esp), %esi; xorl (((65 -14)&0xF)*4)(%esp), %esi; xorl (((65 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebp; movl %esi, ((65&0xF)*4)(%esp); movl %ebx, %esi; xorl %ecx, %esi; xorl %edx, %esi; roll $30, %ebx; leal 0xCA62C1D6(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((66 - 3)&0xF)*4)(%esp), %esi; xorl (((66 - 8)&0xF)*4)(%esp), %esi; xorl (((66 -14)&0xF)*4)(%esp), %esi; xorl (((66 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((66&0xF)*4)(%esp); movl %eax, %esi; xorl %ebx, %esi; xorl %ecx, %esi; roll $30, %eax; leal 0xCA62C1D6(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((67 - 3)&0xF)*4)(%esp), %esi; xorl (((67 - 8)&0xF)*4)(%esp), %esi; xorl (((67 -14)&0xF)*4)(%esp), %esi; xorl (((67 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((67&0xF)*4)(%esp); movl %ebp, %esi; xorl %eax, %esi; xorl %ebx, %esi; roll $30, %ebp; leal 0xCA62C1D6(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((68 - 3)&0xF)*4)(%esp), %esi; xorl (((68 - 8)&0xF)*4)(%esp), %esi; xorl (((68 -14)&0xF)*4)(%esp), %esi; xorl (((68 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((68&0xF)*4)(%esp); movl %edx, %esi; xorl %ebp, %esi; xorl %eax, %esi; roll $30, %edx; leal 0xCA62C1D6(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((69 - 3)&0xF)*4)(%esp), %esi; xorl (((69 - 8)&0xF)*4)(%esp), %esi; xorl (((69 -14)&0xF)*4)(%esp), %esi; xorl (((69 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((69&0xF)*4)(%esp); movl %ecx, %esi; xorl %edx, %esi; xorl %ebp, %esi; roll $30, %ecx; leal 0xCA62C1D6(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (((70 - 3)&0xF)*4)(%esp), %esi; xorl (((70 - 8)&0xF)*4)(%esp), %esi; xorl (((70 -14)&0xF)*4)(%esp), %esi; xorl (((70 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebp; movl %esi, ((70&0xF)*4)(%esp); movl %ebx, %esi; xorl %ecx, %esi; xorl %edx, %esi; roll $30, %ebx; leal 0xCA62C1D6(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((71 - 3)&0xF)*4)(%esp), %esi; xorl (((71 - 8)&0xF)*4)(%esp), %esi; xorl (((71 -14)&0xF)*4)(%esp), %esi; xorl (((71 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((71&0xF)*4)(%esp); movl %eax, %esi; xorl %ebx, %esi; xorl %ecx, %esi; roll $30, %eax; leal 0xCA62C1D6(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((72 - 3)&0xF)*4)(%esp), %esi; xorl (((72 - 8)&0xF)*4)(%esp), %esi; xorl (((72 -14)&0xF)*4)(%esp), %esi; xorl (((72 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((72&0xF)*4)(%esp); movl %ebp, %esi; xorl %eax, %esi; xorl %ebx, %esi; roll $30, %ebp; leal 0xCA62C1D6(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((73 - 3)&0xF)*4)(%esp), %esi; xorl (((73 - 8)&0xF)*4)(%esp), %esi; xorl (((73 -14)&0xF)*4)(%esp), %esi; xorl (((73 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((73&0xF)*4)(%esp); movl %edx, %esi; xorl %ebp, %esi; xorl %eax, %esi; roll $30, %edx; leal 0xCA62C1D6(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((74 - 3)&0xF)*4)(%esp), %esi; xorl (((74 - 8)&0xF)*4)(%esp), %esi; xorl (((74 -14)&0xF)*4)(%esp), %esi; xorl (((74 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((74&0xF)*4)(%esp); movl %ecx, %esi; xorl %edx, %esi; xorl %ebp, %esi; roll $30, %ecx; leal 0xCA62C1D6(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;
 movl (((75 - 3)&0xF)*4)(%esp), %esi; xorl (((75 - 8)&0xF)*4)(%esp), %esi; xorl (((75 -14)&0xF)*4)(%esp), %esi; xorl (((75 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebp; movl %esi, ((75&0xF)*4)(%esp); movl %ebx, %esi; xorl %ecx, %esi; xorl %edx, %esi; roll $30, %ebx; leal 0xCA62C1D6(%ebp,%esi), %ebp; movl %eax, %esi; roll $5, %esi; addl %esi, %ebp;
 movl (((76 - 3)&0xF)*4)(%esp), %esi; xorl (((76 - 8)&0xF)*4)(%esp), %esi; xorl (((76 -14)&0xF)*4)(%esp), %esi; xorl (((76 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %edx; movl %esi, ((76&0xF)*4)(%esp); movl %eax, %esi; xorl %ebx, %esi; xorl %ecx, %esi; roll $30, %eax; leal 0xCA62C1D6(%edx,%esi), %edx; movl %ebp, %esi; roll $5, %esi; addl %esi, %edx;
 movl (((77 - 3)&0xF)*4)(%esp), %esi; xorl (((77 - 8)&0xF)*4)(%esp), %esi; xorl (((77 -14)&0xF)*4)(%esp), %esi; xorl (((77 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ecx; movl %esi, ((77&0xF)*4)(%esp); movl %ebp, %esi; xorl %eax, %esi; xorl %ebx, %esi; roll $30, %ebp; leal 0xCA62C1D6(%ecx,%esi), %ecx; movl %edx, %esi; roll $5, %esi; addl %esi, %ecx;
 movl (((78 - 3)&0xF)*4)(%esp), %esi; xorl (((78 - 8)&0xF)*4)(%esp), %esi; xorl (((78 -14)&0xF)*4)(%esp), %esi; xorl (((78 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %ebx; movl %esi, ((78&0xF)*4)(%esp); movl %edx, %esi; xorl %ebp, %esi; xorl %eax, %esi; roll $30, %edx; leal 0xCA62C1D6(%ebx,%esi), %ebx; movl %ecx, %esi; roll $5, %esi; addl %esi, %ebx;
 movl (((79 - 3)&0xF)*4)(%esp), %esi; xorl (((79 - 8)&0xF)*4)(%esp), %esi; xorl (((79 -14)&0xF)*4)(%esp), %esi; xorl (((79 -16)&0xF)*4)(%esp), %esi; roll $1, %esi; addl %esi, %eax; movl %esi, ((79&0xF)*4)(%esp); movl %ecx, %esi; xorl %edx, %esi; xorl %ebp, %esi; roll $30, %ecx; leal 0xCA62C1D6(%eax,%esi), %eax; movl %ebx, %esi; roll $5, %esi; addl %esi, %eax;


 movl 84(%esp), %esi
 addl %eax, 0(%esi)
 addl %ebx, 4(%esi)
 addl %ecx, 8(%esi)
 addl %edx, 12(%esi)
 addl %ebp, 16(%esi)


 movl 64(%esp), %ebx
 movl 68(%esp), %esi
 movl 72(%esp), %edi
 movl 76(%esp), %ebp
 addl $80, %esp
 retl
